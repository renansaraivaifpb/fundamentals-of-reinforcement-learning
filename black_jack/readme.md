# Agente de Blackjack com Aprendizado por Refor√ßo

[![Python Version](https://img.shields.io/badge/Python-3.9+-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-green.svg)](https://opensource.org/licenses/MIT)

Um agente de IA que aprende a estrat√©gia √≥tima do Blackjack do zero, utilizando o m√©todo *Monte Carlo com Exploring Starts* e a biblioteca Gymnasium do Farama Foundation.

---

## üìñ Sum√°rio

Este projeto explora uma quest√£o fundamental do Aprendizado por Refor√ßo (RL): como um agente pode dominar um jogo de probabilidades como o Blackjack sem conhecimento pr√©vio de suas regras ou estrat√©gias?

O objetivo √© construir e treinar um agente capaz de desenvolver uma pol√≠tica de decis√£o √≥tima (`œÄ*`) unicamente atrav√©s da experi√™ncia adquirida ao jogar milhares de partidas. A solu√ß√£o √© uma implementa√ß√£o em Python do algoritmo **Monte Carlo com Exploring Starts**, um m√©todo *model-free* que aprende o valor de cada a√ß√£o em cada estado do jogo.

## üÉè O Desafio: Dominando o Blackjack

No Blackjack, o objetivo √© vencer a m√£o do Dealer (a Mesa) sem ultrapassar 21 pontos. O agente precisa tomar decis√µes sequenciais (Pedir carta ou Parar) com base em sua soma atual, na carta vis√≠vel do Dealer e na presen√ßa de um √Ås utiliz√°vel. Este ambiente oferece um cen√°rio cl√°ssico para problemas de RL, com um balan√ßo claro entre risco e recompensa.

## üß† Metodologia: Aprendizado por Refor√ßo

A aprendizagem do agente √© guiada pelo princ√≠pio da **Itera√ß√£o de Pol√≠tica Generalizada (GPI)**, um ciclo cont√≠nuo de avalia√ß√£o e melhoria da pol√≠tica.

1.  **Avalia√ß√£o da Pol√≠tica (Policy Evaluation):** O agente joga epis√≥dios completos (partidas) com uma pol√≠tica inicial. Ao final de cada epis√≥dio, o retorno (recompensa: +1 para vit√≥ria, -1 para derrota, 0 para empate) √© calculado e propagado para tr√°s, atualizando o valor esperado para cada par (estado, a√ß√£o) visitado. O valor de uma a√ß√£o em um estado, $Q(s, a)$, √© simplesmente a m√©dia de todos os retornos recebidos ap√≥s tomar aquela a√ß√£o naquele estado.

2.  **Melhoria da Pol√≠tica (Policy Improvement):** Ap√≥s cada epis√≥dio, a pol√≠tica √© atualizada de forma "gananciosa" (*greedy*). Para cada estado, o agente passa a escolher a a√ß√£o que possui o maior valor $Q$ estimado at√© o momento.

Este ciclo de `jogar -> avaliar -> melhorar` √© a engrenagem que, ao longo de milhares de epis√≥dios, leva o agente a convergir para uma pol√≠tica √≥tima. A f√≥rmula de atualiza√ß√£o para o valor Q a cada retorno $G_t$ observado √©:

$$ Q(s, a) \leftarrow Q(s, a) + \alpha [G_t - Q(s, a)] $$

*(Nesta implementa√ß√£o "first-visit", Œ± √© efetivamente 1/N(s,a), onde N √© a contagem de visitas, pois calculamos a m√©dia direta dos retornos).*

## üõ†Ô∏è Pilha Tecnol√≥gica

-   **Simula√ß√£o de Ambiente:** `gymnasium`
-   **Computa√ß√£o Num√©rica:** `numpy`
-   **Manipula√ß√£o de Dados:** `pandas`
-   **Visualiza√ß√£o de Dados:** `matplotlib`
-   **Barra de Progresso:** `tqdm`

## ‚öôÔ∏è Instala√ß√£o

Para executar este projeto, voc√™ precisa ter o Python 3.9+ instalado.

1.  **Clone o reposit√≥rio:**
    ```bash
    git clone [URL_DO_SEU_REPOSITORIO_GITLAB]
    cd [NOME_DO_PROJETO]
    ```

2.  **Crie e ative um ambiente virtual:**
    ```bash
    python -m venv venv
    source venv/bin/activate
    # No Windows, use: venv\Scripts\activate
    ```

3.  **Instale as depend√™ncias a partir do `requirements.txt`:**
    ```bash
    pip install -r requirements.txt
    ```
    *(Nota: Certifique-se de criar um arquivo `requirements.txt` com as bibliotecas listadas na Pilha Tecnol√≥gica).*

## ‚ñ∂Ô∏è Uso

Para iniciar o treinamento do agente, execute o script principal:

```bash
python main.py
```

O script ir√° treinar o agente por 500.000 epis√≥dios, exibindo o progresso e a pol√≠tica aprendida para cen√°rios espec√≠ficos a cada 50.000 epis√≥dios. Ao final do treinamento, dois gr√°ficos ser√£o gerados e exibidos.

## üìä Resultados e An√°lise

Ap√≥s 500.000 epis√≥dios, o agente convergiu para uma pol√≠tica est√°vel, descobrindo 280 estados de jogo √∫nicos e alcan√ßando uma recompensa m√©dia de **-0.2533** nos √∫ltimos 50.000 jogos ‚Äî um resultado robusto que se aproxima do desempenho √≥timo.

```
100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500000/500000 [05:01<00:00, 1660.85it/s]

--- Progresso no Epis√≥dio: 500000/500000 ---
  Recompensa M√©dia (√∫ltimos 50000 epis√≥dios): -0.2533
  Tamanho da Pol√≠tica (estados conhecidos): 280
------------------------------------------------------------
  Pol√≠tica Aprendida para Cen√°rios Monitorados:
    - Estado (20, 10, False)          : A√ß√£o -> Stick
    - Estado (12, 3, False)           : A√ß√£o -> Hit
    - Estado (16, 10, False)          : A√ß√£o -> Stick
    - Estado (13, 2, False)           : A√ß√£o -> Stick
    - Estado (17, 9, False)           : A√ß√£o -> Stick
    - Estado (19, 10, True)           : A√ß√£o -> Stick
    - Estado (17, 6, True)            : A√ß√£o -> Hit
    - Estado (15, 4, True)            : A√ß√£o -> Hit
    - Estado (13, 10, True)           : A√ß√£o -> Hit
    - Estado (21, 1, False)           : A√ß√£o -> Stick
------------------------------------------------------------
```

### Intelig√™ncia Descoberta: An√°lise da Pol√≠tica

A pol√≠tica final revela decis√µes estrat√©gicas complexas que n√£o foram pr√©-programadas:

-   **Entendimento do "Soft 17":** O agente aprendeu a jogada cr√≠tica (e n√£o √≥bvia) de pedir carta (*Hit*) com um "soft 17" (√Ås+6) contra um 6 do Dealer, aproveitando a flexibilidade do √Ås para melhorar sua m√£o sem risco de estourar.
-   **Gest√£o de Risco:** Em situa√ß√µes desfavor√°veis, como ter 16 contra um 10 do Dealer, o agente aprendeu a parar (*Stick*), adotando uma pol√≠tica conservadora para minimizar a alta probabilidade de ultrapassar 21.

O gr√°fico de progresso de aprendizagem mostra a converg√™ncia da recompensa m√©dia ao longo do tempo.

![Progresso de Aprendizagem](assets/learning_progress.png)

A pol√≠tica final √© visualizada abaixo. Cada mapa de calor mostra a a√ß√£o ideal (Amarelo=Hit, Roxo=Stick) para cada cen√°rio, distinguindo entre m√£os com e sem um √Ås utiliz√°vel.

![Pol√≠tica √ìtima Aprendida](assets/final_policy.png)

Nota-se que o agente √© muito mais agressivo quando possui um √Ås utiliz√°vel, pois compreendeu que ele atua como uma "rede de seguran√ßa", eliminando o risco imediato de estourar e abrindo oportunidades para melhorar a m√£o.

### Conex√£o com Engenharia de Controle e Automa√ß√£o

Este projeto serve como uma prova de conceito para a aplica√ß√£o de abordagens de RL em problemas de otimiza√ß√£o. A mesma l√≥gica usada para otimizar decis√µes em um jogo pode ser transposta para otimizar pol√≠ticas de controle em sistemas do mundo real, como na gest√£o de energia de edif√≠cios, no controle de processos industriais ou na calibra√ß√£o de sistemas aut√¥nomos, onde o objetivo √© maximizar uma recompensa (efici√™ncia, produ√ß√£o) ao longo do tempo.

## üìÑ Licen√ßa

Este projeto est√° licenciado sob a Licen√ßa MIT. Veja o arquivo [LICENSE](LICENSE) para mais detalhes.
